# GELU
Gaussian Error Linear Unit. An activation function used in the most recent Transformers – Google's BERT and OpenAI's GPT-2. The paper is from 2016, but is only catching attention up until recently.
## Function
$$
   F(x) = \frac{1}{2} x (1 + erf(\frac{x}{\sqrt{2}})
$$
$$
	= \alpha( x),\quad  x\leq0
$$
## Graph

![GELU Graph](https://github.com/UvrajSB/Machine-Learning-concepts/blob/main/non-linearities/GELU/GELU.png)
