# GELU
Gaussian Error Linear Unit. An activation function used in the most recent Transformers – Google's BERT and OpenAI's GPT-2. The paper is from 2016, but is only catching attention up until recently.
## Function

![GELU function](https://github.com/alishdipani/Machine-Learning-concepts/blob/4a7839fe74b7662e2c80739fea0a7bc38037260c/non-linearities/GELU/Function%20of%20GELU.png)

## Graph

![GELU Graph](https://github.com/UvrajSB/Machine-Learning-concepts/blob/main/non-linearities/GELU/GELU.png)
